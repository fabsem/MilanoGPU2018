{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "\n",
    "import pycuda.driver as cuda_driver\n",
    "import pycuda.compiler as cuda_compiler\n",
    "from pycuda.gpuarray import GPUArray\n",
    "\n",
    "import IPythonMagic\n",
    "from Timer import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python version 3.6.6 (default, Sep 12 2018, 18:26:19) \n",
      "[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]]\n"
     ]
    }
   ],
   "source": [
    "%setup_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registering context in user workspace\n",
      "Creating context\n",
      "PyCUDA version 2018.1.1\n",
      "CUDA version (9, 1, 0)\n",
      "Driver version 10000\n",
      "Using 'Tesla K80' GPU\n",
      " => compute capability: (3, 7)\n",
      " => memory: 10328 / 11441 MB available\n",
      "Created context handle <49174640>\n",
      "Using CUDA cache dir /home/ubuntu/jupyter_notebooks/Fabio/MilanoGPU2018/notebooks/cuda_cache\n"
     ]
    }
   ],
   "source": [
    "%cuda_context_handler context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_src= \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "__global__ void shmemReduction(float* output, float* input, int size) {\n",
    "    //First we stride throug global memory and compute\n",
    "    //the maximum for every thread\n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x; //blockIdx.x is always zero because we use just one block!\n",
    "    \n",
    "    float max_value = -9999999.99; //FIXME: USE PROPER NUMBER\n",
    "    for (int i = threadIdx.x; i < size; i = i + blockDim.x) { //this gives the nice memory accesso\n",
    "    \n",
    "    max_value = fmaxf(max_value, input[i]); \n",
    "}\n",
    "\n",
    "    //Temporary write to memory to check if things work so far\n",
    "    output[threadIdx.x] = max_value;\n",
    "\n",
    "\n",
    "    //Store the per-thread maximum in shared memory\n",
    "    __shared__ float max_shared[128];\n",
    "    max_shared[threadIdx.x] = max_value;\n",
    "\n",
    "\n",
    "    //Synchronize so that all thread see the same shared memory\n",
    "    __syncthreads();\n",
    "\n",
    "\n",
    "    //Find the maximum in shared memory\n",
    "    \n",
    "    //Reduce from 128 to 64 elements\n",
    "    \n",
    "    if (threadIdx.x < 64) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 64]);\n",
    "    }\n",
    "    \n",
    "    //since we here have more than one active warp(threadIdx.x > 32)\n",
    "    //We need to make sure all threads have finished before continuing\n",
    "    __syncthreads();\n",
    "    \n",
    "    //Reduce from 64 to 32 elements\n",
    "    \n",
    "    if (threadIdx.x < 32) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 32]);\n",
    "    }\n",
    "    \n",
    "    //Reduce from 32 to 16 elements\n",
    "    \n",
    "    if (threadIdx.x < 16) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 16]);\n",
    "    }\n",
    "    \n",
    "    //Reduce from 16 to 8 elements\n",
    "    if (threadIdx.x < 8) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 8]);\n",
    "    }\n",
    "    \n",
    "    //Reduce from 8 to 4 elements\n",
    "    if (threadIdx.x < 4) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 4]);\n",
    "    }\n",
    "    \n",
    "    //Reduce from 4 to 2 elements\n",
    "    if (threadIdx.x < 2) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 2]);\n",
    "    }\n",
    "    \n",
    "    //Reduce from 2 to 1 elements\n",
    "    if (threadIdx.x < 1) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 1]);\n",
    "    }\n",
    "    //Finally write out to output\n",
    "    \n",
    "    if (threadIdx.x == 0) {\n",
    "    output[0] = max_shared[0];\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "kernel_module = cuda_compiler.SourceModule(kernel_src)\n",
    "kernel_function = kernel_module.get_function(\"shmemReduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6881449  0.72943795 0.5265379  0.0485927  0.47310767 0.26235375\n",
      "  0.30650806 0.81399363 0.62254995 0.11792846 0.19896875 0.53583235\n",
      "  0.1757224  0.10557467 0.22648853 0.23038836 0.29722062 0.3988975\n",
      "  0.45321867 0.7799348  0.78821105 0.39444345 0.2839206  0.15808147\n",
      "  0.71925986 0.12927413 0.19470625 0.11385528 0.40766597 0.6649255\n",
      "  0.8532714  0.3278626  0.26319018 0.3125056  0.81573933 0.03239956\n",
      "  0.32090753 0.08855749 0.12553832 0.62899595 0.8279062  0.4880188\n",
      "  0.06961014 0.6205518  0.9780777  0.5417944  0.07341954 0.16479595\n",
      "  0.73759574 0.41091383 0.9695671  0.53390026 0.9214707  0.44894105\n",
      "  0.5145385  0.69200647 0.80482817 0.7688912  0.7003725  0.90557766\n",
      "  0.36182457 0.23052981 0.21603042 0.35390586 0.86607    0.07634681\n",
      "  0.54980123 0.4596523  0.71344626 0.15875824 0.76641756 0.3753674\n",
      "  0.6163961  0.1158151  0.5898228  0.5116801  0.08103568 0.00196694\n",
      "  0.4242701  0.81932425 0.9008657  0.9965997  0.5672313  0.66611177\n",
      "  0.8842715  0.51376545 0.33707413 0.29498452 0.992028   0.29007104\n",
      "  0.40823585 0.8603877  0.9845898  0.339643   0.93033475 0.47615185\n",
      "  0.5905528  0.7108138  0.52723485 0.9245107  0.40436012 0.88170147\n",
      "  0.614155   0.6411196  0.23040377 0.6453051  0.9763274  0.26539147\n",
      "  0.94495064 0.52589554 0.07785533 0.6801322  0.39929575 0.86662686\n",
      "  0.89246064 0.703105   0.6428399  0.8868531  0.36847332 0.86738265\n",
      "  0.28957778 0.8539445  0.9881176  0.60425025 0.8184917  0.37814116\n",
      "  0.312714   0.5204584  0.68666786 0.7368305  0.5794553  0.72613806\n",
      "  0.5702818  0.27844715 0.8637804  0.13316445 0.868888   0.64477724\n",
      "  0.8366947  0.71680295 0.53592104 0.5691639  0.8900309  0.8735503\n",
      "  0.04340415 0.06722874 0.623437   0.31431904 0.9472156  0.2788798\n",
      "  0.03963127 0.4216103  0.8807854  0.90175635 0.46937704 0.79452634\n",
      "  0.16157795 0.81355345 0.7932481  0.28872126 0.71762216 0.05222984\n",
      "  0.238205   0.2075417  0.3590663  0.5696483  0.95251024 0.08859602\n",
      "  0.4713152  0.79883236 0.802878   0.33967668 0.8920558  0.351809\n",
      "  0.31218296 0.21474954 0.6619812  0.06257853 0.5274264  0.35568896\n",
      "  0.31661323 0.56683475 0.5945827  0.75553596 0.19055073 0.19212729\n",
      "  0.42155936 0.4266182  0.05606062 0.34562343 0.9640327  0.15837097\n",
      "  0.36901927 0.16528499 0.8268355  0.24519612 0.76723313 0.53746384\n",
      "  0.9145336  0.73120236 0.86825883 0.52703947 0.2323644  0.986169\n",
      "  0.5220367  0.6235114  0.4934892  0.03134594 0.7973942  0.64291316\n",
      "  0.28926477 0.14296341 0.88954484 0.5240076  0.59408945 0.8113328\n",
      "  0.4933034  0.37220967 0.45529142 0.7854857  0.16701159 0.05032863\n",
      "  0.21221963 0.8635176  0.32290086 0.60423636 0.7389895  0.01979898\n",
      "  0.14340685 0.42795923 0.538118   0.7513592  0.58219993 0.86475253\n",
      "  0.9715559  0.83772093 0.31899515 0.2970511  0.24124229 0.90291667\n",
      "  0.611749   0.13537377 0.11800356 0.01953612 0.8939891  0.94560874\n",
      "  0.68531066 0.2318074  0.983591   0.7939895  0.65750885 0.43680793\n",
      "  0.46527082 0.22571573 0.05216147 0.8527918 ]]\n",
      "0.9965997\n"
     ]
    }
   ],
   "source": [
    "n = 256\n",
    "a = np.random.random((1, n)).astype(np.float32)\n",
    "print(a)\n",
    "\n",
    "a_g = GPUArray(a.shape, a.dtype)\n",
    "a_g.set(a)\n",
    "\n",
    "num_threads = 128 #each thread the max of 2 numbers\n",
    "b = np.empty((1, num_threads), dtype=np.float32)\n",
    "\n",
    "b_g = GPUArray(b.shape, b.dtype)\n",
    "\n",
    "block_size = (num_threads, 1, 1)\n",
    "grid_size = (1, 1, 1)\n",
    "\n",
    "kernel_function(b_g, a_g, np.int32(n), grid=grid_size, block = block_size)\n",
    "\n",
    "b_g.get(b)\n",
    "\n",
    "#print(a)\n",
    "#print(b)\n",
    "print(np.max(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
